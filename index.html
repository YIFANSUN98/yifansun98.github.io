<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta and Tairan He*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="images/ri-new-mark-512-512-transparent.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Yifan Sun</title>
  <meta name="Yifan Sun's Homepage" http-equiv="Content-Type" content="Yifan Sun's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>
 
<body>
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Yifan Sun 孙逸凡</pageheading><br>
  </p>

  <tr>
    <td width="30%" valign="top"><a href="images/YifanPhoto.jpg"><img src="images/YifanPhoto.jpg" width="100%" style="border-radius:15px"></a>
    <p align=center>
    | <a href="data/Yifan_CV_09032024.pdf">CV</a> |
    <a href="mailto: yifansu2@andrew.cmu.edu">Email</a> |
    <a href="https://scholar.google.com/citations?user=DGhQSYUAAAAJ&hl=en">Google Scholar</a> |
    <br/>
    | <a href="https://github.com/YIFANSUN98">Github</a> | 
    <a href="https://www.linkedin.com/in/yifansun1/">LinkedIn</a> |
    </p>
    <p align="center">
      <a href="https://twitter.com/YifanSun98?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @YifanSun98</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
    </p> 
    </td>
    <td width="70%" valign="top" align="justify">
      <p>I am a first year Ph.D. student at the <a href="https://www.ri.cmu.edu">Robotics Institute</a> at <a href="https://www.cmu.edu">Carnegie Mellon University</a>, advised by <a href="https://www.cs.cmu.edu/~cliu6/"> Changliu Liu</a>.
      </p>
      <p>I received my Bachelor's degree in Mechanical Engineering at <a href="http://en.xjtu.edu.cn/">Xi'an Jiaotong University</a> and got my Masters's drgess in Mechanical Engineering at CMU. 
      </p>
      <p>I also had the pleasure of completing two summer internships at <a href="https://zoox.com/"> Zoox</a> and <a href="https://www.siemens.com/us/en.html?gclid=CjwKCAjw59q2BhBOEiwAKc0ijbLAsbTVBrGqt0c6VOFrFe_tD9yP6UCLV4Y-1cAXWJ-0ngmoKIEOxxoC810QAvD_BwE&acz=1&gad_source=1"> Siemens</a>.
      </p>
        <!-- <p>Goal: challenge conventional notions of what robots can achieve, develop robots that improves everyone's life. Focus: developing intelligent robots being able to do useful tasks with <u>intelligence, generalizability, agility and safety</u>. Method: learning-based methods that scale with the computation and data. Robots: Mobile robots, legged robots, robotic manipulators, and humanoid robots.
      </p> -->
      <!-- <p><strong>Goal:</strong> Robots that improve everyone's life.</p>
      <p><strong>Focus:</strong> How to build the <u>data flywheel for robotics</u>? How to make robots perform useful tasks with <u>intelligence, generalizability, agility, and safety</u>?</p>
      <p><strong>Method:</strong> Utilizing learning-based methods that scale with computation and data.</p>
      <p><strong>Robots:</strong> I love working on humanoids and aim to make them capable of doing everything I can do—and more.</p> -->
      <p>Email: yifansu2@andrew.cmu.edu
      </p>
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <!-- Koopman -->
  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/pdf/2411.14321">
          <img src="images/koopman/koopman.png" alt="sym" width="100%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/pdf/2411.14321" id="KOOPMAN">
      <heading>Continual Learning and Lifting of Koopman Dynamics for Linear Control of Legged Robots</heading></a><br>
      Feihan Li, Abulikemu Abuduweili, <b>Yifan Sun</b>, Rui Chen, Weiye Zhao, Changliu Liu<br>
      2024<br>
      </p>

      <div class="paper" id="koopman">
      <a href="javascript:toggleblock('koopman_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('koopman')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2411.14321">arXiv</a> 

      <p align="justify"> <i id="koopman_abs">The control of legged robots, particularly humanoid and quadruped robots, presents significant challenges due to their high-dimensional and nonlinear dynamics. While linear systems can be effectively controlled using methods like Model Predictive Control (MPC), the control of nonlinear systems remains complex. One promising solution is the Koopman Operator, which approximates nonlinear dynamics with a linear model, enabling the use of proven linear control techniques. However, achieving accurate linearization through data-driven methods is difficult due to issues like approximation error, domain shifts, and the limitations of fixed linear state-space representations. These challenges restrict the scalability of Koopman-based approaches. This paper addresses these challenges by proposing a continual learning algorithm designed to iteratively refine Koopman dynamics for high-dimensional legged robots. The key idea is to progressively expand the dataset and latent space dimension, enabling the learned Koopman dynamics to converge towards accurate approximations of the true system dynamics. Theoretical analysis shows that the linear approximation error of our method converges monotonically. Experimental results demonstrate that our method achieves high control performance on robots like Unitree G1/H1/A1/Go2 and ANYmal D, across various terrains using simple linear MPC controllers. This work is the first to successfully apply linearized Koopman dynamics for locomotion control of high-dimensional legged robots, enabling a scalable model-based control solution.</i></p>

      <pre xml:space="preserve">
        @article{li2024continual,
          title={Continual Learning and Lifting of Koopman Dynamics for Linear Control of Legged Robots},
          author={Li, Feihan and Abuduweili, Abulikemu and Sun, Yifan and Chen, Rui and Zhao, Weiye and Liu, Changliu},
          journal={arXiv preprint arXiv:2411.14321},
          year={2024}
        }
      </pre>
      </div>
    </td>
  </tr>

  <!-- Lego Manipulation -->
  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/pdf/2309.02354">
          <img src="images/lego_manipulation/lego_manipulation.gif" alt="sym" width="100%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/pdf/2309.02354" id="LegoManipulation">
      <heading>A Lightweight and Transferable Design for Robust LEGO Manipulation</heading></a><br>
      Ruixuan Liu, <b>Yifan Sun</b>, Changliu Liu<br>
      International Symposium on Flexible Automation (ISFA), 2024<br>
      </p>

      <div class="paper" id="lego_manipulation">
      <a href="https://www.youtube.com/watch?v=CqrBryAGsd0">youtube</a> |
      <a href="javascript:toggleblock('lego_manipulation_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('lego_manipulation')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2309.02354">arXiv</a> 

      <p align="justify"> <i id="lego_manipulation_abs">Lego is a well-known platform for prototyping pixelized objects. However, robotic Lego prototyping (i.e., manipulating Lego bricks) is challenging due to the tight connections and accuracy requirements. This paper investigates safe and efficient robotic Lego manipulation. In particular, this paper reduces the complexity of the manipulation by hardware-software co-design. An end-of-arm tool (EOAT) is designed, which reduces the problem dimension and allows large industrial robots to manipulate small Lego bricks. In addition, this paper uses evolution strategy to optimize the robot motion for Lego manipulation. Experiments demonstrate that the EOAT can reliably manipulate Lego bricks and the learning framework can effectively and safely improve the manipulation performance to a 100% success rate. The co-design is deployed to multiple robots (i.e., FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its generalizability and transferability. In the end, we show that the proposed solution enables sustainable robotic Lego prototyping, in which the robot can repeatedly assemble and disassemble different prototypes.</i></p>

      <pre xml:space="preserve">
        @inproceedings{liu2024lightweight,
          title={A lightweight and transferable design for robust lego manipulation},
          author={Liu, Ruixuan and Sun, Yifan and Liu, Changliu},
          booktitle={International Symposium on Flexible Automation},
          volume={87882},
          pages={V001T07A004},
          year={2024},
          organization={American Society of Mechanical Engineers}
        }
      </pre>
      </div>
    </td>
  </tr>

  <!-- APO -->
  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/pdf/2305.13681">
          <img src="images/apo/apo.png" alt="sym" width="100%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/pdf/2305.13681" id="APO">
      <heading>Absolute Policy Optimization: Enhancing Lower Probability Bound of Performance with High Confidence</heading></a><br>
      Weiye Zhao*, Feihan Li*, <b>Yifan Sun</b>, Rui Chen, Tianhao Wei, Changliu Liu<br>
      International Conference on Machine Learning (ICML), 2024<br>
      </p>

      <div class="paper" id="apo">
      <a href="javascript:toggleblock('apo_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('apo')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2305.13681">arXiv</a> 

      <p align="justify"> <i id="apo_abs">In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function, optimizing which leads to guaranteed monotonic improvement in the lower probability bound of performance with high confidence. Building upon this groundbreaking theoretical advancement, we further introduce a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO as well as its efficient variation Proximal Absolute Policy Optimization (PAPO) significantly outperforms state-of-the-art policy gradient algorithms, resulting in substantial improvements in worst-case performance, as well as expected performance.</i></p>

      <pre xml:space="preserve">
        @inproceedings{zhaoabsolute,
          title={Absolute Policy Optimization: Enhancing Lower Probability Bound of Performance with High Confidence},
          author={Zhao, Weiye and Li, Feihan and Sun, Yifan and Chen, Rui and Wei, Tianhao and Liu, Changliu},
          booktitle={Forty-first International Conference on Machine Learning}
        }
      </pre>
      </div>
    </td>
  </tr>

  <!-- SCPO -->
  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/pdf/2305.13681">
          <img src="images/scpo/scpo.png" alt="sym" width="100%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/pdf/2305.13681" id="SCPO">
      <heading>State-wise Constrained Policy Optimization</heading></a><br>
      Weiye Zhao, Rui Chen, <b>Yifan Sun</b>, Tianhao Wei, Changliu Liu<br>
      Transactions on Machine Learning Research (TMLR), 2024<br>
      </p>

      <div class="paper" id="scpo">
      <a href="javascript:toggleblock('scpo_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('scpo')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2305.13681">arXiv</a> 

      <p align="justify"> <i id="scpo_abs">Reinforcement Learning (RL) algorithms have shown tremendous success in simulation environments, but their application to real-world problems faces significant challenges, with safety being a major concern. In particular, enforcing state-wise constraints is essential for many challenging tasks such as autonomous driving and robot manipulation. However, existing safe RL algorithms under the framework of Constrained Markov Decision Process (CMDP) do not consider state-wise constraints. To address this gap, we propose State-wise Constrained Policy Optimization (SCPO), the first general-purpose policy search algorithm for state-wise constrained reinforcement learning. SCPO provides guarantees for state-wise constraint satisfaction in expectation. In particular, we introduce the framework of Maximum Markov Decision Process, and prove that the worst-case safety violation is bounded under SCPO. We demonstrate the effectiveness of our approach on training neural network policies for extensive robot locomotion tasks, where the agent must satisfy a variety of state-wise safety constraints. Our results show that SCPO significantly outperforms existing methods and can handle state-wise constraints in high-dimensional robotics tasks.</i></p>

      <pre xml:space="preserve">
        @article{zhao2023state,
          title={State-wise constrained policy optimization},
          author={Zhao, Weiye and Chen, Rui and Sun, Yifan and Wei, Tianhao and Liu, Changliu},
          journal={arXiv preprint arXiv:2306.12594},
          year={2023}
        }
      </pre>
      </div>
    </td>
  </tr>

  <!-- Guard -->
  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/pdf/2305.13681">
          <img src="images/guard/guard.png" alt="sym" width="100%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/pdf/2305.13681" id="GUARD">
      <heading>GUARD: A Safe Reinforcement Learning Benchmark</heading></a><br>
      Weiye Zhao*, <b>Yifan Sun</b>*, Feihan Li, Rui Chen, Ruixuan Liu, Tianhao Wei, Changliu Liu<br>
      Transactions on Machine Learning Research (TMLR), 2024<br>
      </p>

      <div class="paper" id="guard">
      <a href="https://github.com/intelligent-control-lab/guard">code</a> |
      <a href="javascript:toggleblock('guard_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('guard')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2305.13681">arXiv</a> 

      <p align="justify"> <i id="guard_abs">Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state-of-the-art safe RL algorithms in various task settings using GUARD and establish baselines that future work can build on.</i></p>

      <pre xml:space="preserve">
        @article{zhao2023guard,
          title={Guard: A safe reinforcement learning benchmark},
          author={Zhao, Weiye and Sun, Yifan and Li, Feihan and Chen, Rui and Liu, Ruixuan and Wei, Tianhao and Liu, Changliu},
          journal={arXiv preprint arXiv:2305.13681},
          year={2023}
        }
      </pre>
      </div>
    </td>
  </tr>

  <!-- HTCP -->
  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/pdf/2304.09260">
          <img src="images/htcp/htcp.png" alt="sym" width="100%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/pdf/2304.09260" id="HTCP">
      <heading>Hybrid Task Constrained Planner for Robot Manipulator in Confined Environment</heading></a><br>
      <b>Yifan Sun</b>, Weiye Zhao, Changliu Liu<br>
      American Control Conference (ACC) 2024<br>
      </p>

      <div class="paper" id="htcp">
      <a href="javascript:toggleblock('htcp_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('htcp')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2304.09260">arXiv</a> 

      <p align="justify"> <i id="htcp_abs">Incremental motion planning has emerged as a powerful approach for generating safe trajectories in confined environments. However, its effectiveness can be hampered by susceptibility to local optima. This vulnerability arises from the algorithm's heavy dependence on the previously achieved configuration (pose) as a reference for the next step. This paper presents a novel incremental motion planning approach for redundant robot arms. It leverages an optimization-based planner combined with null-space exploration to escape local optima and generate high-quality trajectories satisfying task and collision constraints. Our approach is evaluated in an onsite polishing scenario with various robot and workpiece configurations, demonstrating significant improvements in trajectory quality compared to existing methods. The proposed approach has the potential for broad applications in industrial tasks involving redundant robot arms.</i></p>

      <pre xml:space="preserve">
        @inproceedings{sun2024hybrid,
          title={Hybrid Task Constrained Incremental Planner for Robot Manipulators in Confined Environments},
          author={Sun, Yifan and Zhao, Weiye and Liu, Changliu},
          booktitle={2024 American Control Conference (ACC)},
          pages={3847--3852},
          year={2024},
          organization={IEEE}
        }
      </pre>
      </div>
    </td>
  </tr>

  <!-- S3PO -->
  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/pdf/2308.13140">
          <img src="images/s3po/s3po.png" alt="sym" width="100%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/pdf/2308.13140" id="S3PO">
      <heading>Learn with imagination: Safe set guided state-wise constrained policy optimization</heading></a><br>
      Feihan Li*, <b>Yifan Sun</b>*, Weiye Zhao*, Rui Chen, Tianhao Wei, Changliu Liu<br>
      2023<br>
      </p>

      <div class="paper" id="s3po">
      <a href="javascript:toggleblock('s3po_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('s3po')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2308.13140">arXiv</a> 

      <p align="justify"> <i id="s3po_abs">Deep reinforcement learning (RL) excels in various control tasks, yet the absence of safety guarantees hampers its real-world applicability. In particular, explorations during learning usually results in safety violations, while the RL agent learns from those mistakes. On the other hand, safe control techniques ensure persistent safety satisfaction but demand strong priors on system dynamics, which is usually hard to obtain in practice. To address these problems, we present Safe Set Guided State-wise Constrained Policy Optimization (S-3PO), a pioneering algorithm generating state-wise safe optimal policies with zero training violations, i.e., learning without mistakes. S-3PO first employs a safety-oriented monitor with black-box dynamics to ensure safe exploration. It then enforces an "imaginary" cost for the RL agent to converge to optimal behaviors within safety constraints. S-3PO outperforms existing methods in high-dimensional robotics tasks, managing state-wise constraints with zero training violation. This innovation marks a significant stride towards real-world safe RL deployment.</i></p>

      <pre xml:space="preserve">
        @article{li2023learn,
          title={Learn with imagination: Safe set guided state-wise constrained policy optimization},
          author={Li, Feihan and Sun, Yifan and Zhao, Weiye and Chen, Rui and Wei, Tianhao and Liu, Changliu},
          journal={arXiv preprint arXiv:2308.13140},
          year={2023}
        }
      </pre>
      </div>
    </td>
  </tr>

  <table width="30%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
        <tr>
            <td style="padding:0px">
                <br>
                <br>
                <div>
                  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=j7pCFMS--HyWHJq-O4KWye7muE9ye6LmWaKOF2-16g0"></script>
                </div>
            </td>
        </tr>
    </tbody>
  </table>

  <!-- Lego2DLFD -->
  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/pdf/2305.15667">
          <img src="images/s3po/s3po.png" alt="sym" width="20%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/pdf/2305.15667" id="Lego2DLFD">
      <heading>Robotic LEGO Assembly and Disassembly from Human Demonstration</heading></a><br>
      Ruixuan Liu, <b>Yifan Sun</b>, Changliu Liu<br>
      ACC Workshop on Recent Advancement of Human Autonomy Interaction and Integration, 2023<br>
      </p>

      <div class="paper" id="lego2dlfd">
      <a href="https://www.youtube.com/watch?v=CRIKbV_0TvU">youtube</a> |
      <a href="javascript:toggleblock('lego2dlfd_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('lego2dlfd')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2305.15667">arXiv</a> 

      <p align="justify"> <i id="lego2dlfd_abs">This paper studies automatic prototyping using LEGO. To satisfy individual needs and self-sustainability, this paper presents a framework that learns the assembly and disassembly sequences from human demonstrations. In addition, a digital twin is developed to verify the correctness of robot learning before deploying to the real world. Moreover, an end-effector tool (EOT) is designed, which allows large industrial robots to easily manipulate LEGO bricks. The proposed system is deployed to a FANUC LR-mate 200id/7L robot. Experiments demonstrate that the proposed system can effectively learn the assembly and disassembly tasks from human demonstrations. And the learned tasks are realized by the FANUC robot.</i></p>

      <pre xml:space="preserve">
        @article{liu2023robotic,
          title={Robotic lego assembly and disassembly from human demonstration},
          author={Liu, Ruixuan and Sun, Yifan and Liu, Changliu},
          journal={arXiv preprint arXiv:2305.15667},
          year={2023}
        }
      </pre>
      </div>
    </td>
  </tr>

  <!-- JPC -->
  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://ieeexplore.ieee.org/document/9863251">
          <img src="images/s3po/s3po.png" alt="sym" width="20%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://ieeexplore.ieee.org/document/9863251" id="JPC">
      <heading>Jerk-bounded position controller with real-time task modification for interactive industrial robots</heading></a><br>
      Ruixuan Liu, Rui Chen, <b>Yifan Sun</b>, Yu Zhao, Changliu Liu<br>
      IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM), 2022<br>
      </p>

      <div class="paper" id="jpc">
      <a href="https://www.youtube.com/watch?v=bha6pHhytZA">youtube</a> |
      <a href="javascript:toggleblock('jpc_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('jpc')" class="togglebib">bibtex</a> |
      <a href="https://ieeexplore.ieee.org/document/9863251">arXiv</a> 

      <p align="justify"> <i id="jpc_abs">Industrial robots are widely used in many applications with structured and deterministic environments. However, the contemporary need requires industrial robots to intelligently operate in dynamic environments. It is challenging to design a safe and efficient robotic system with industrial robots in a dynamic environment for several reasons. First, most industrial robots require the input to have specific formats, which takes additional efforts to convert from task-level user commands. Second, existing robot drivers do not support overwriting ongoing tasks in real-time, which hinders the robot from responding to the dynamic environment. Third, most industrial robots only expose motion-level control, making it challenging to enforce dynamic constraints during trajectory tracking. To resolve the above challenges, this paper presents a jerk-bounded position control driver (JPC) for industrial robots. JPC provides a unified interface for tracking complex trajectories and is able to enforce dynamic constraints using motion-level control, without accessing servo-level control. Most importantly, JPC enables real-time trajectory modification. Users can overwrite the ongoing task with a new one without violating dynamic constraints. The proposed JPC is implemented and tested on the FANUC LR Mate 200id/7L robot with both artificially generated data and an interactive robot handover task. Experiments show that the proposed JPC can track complex trajectories accurately within dynamic limits and seamlessly switch to new trajectory references before the ongoing task ends.</i></p>

      <pre xml:space="preserve">
        @inproceedings{liu2022jerk,
          title={Jerk-bounded position controller with real-time task modification for interactive industrial robots},
          author={Liu, Ruixuan and Chen, Rui and Sun, Yifan and Zhao, Yu and Liu, Changliu},
          booktitle={2022 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)},
          pages={1771--1778},
          year={2022},
          organization={IEEE}
        }
      </pre>
      </div>
    </td>
  </tr>

  <!-- <table width="30%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
        <tr>
            <td style="padding:0px">
                <br>
                <br>
                <div>
                  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=j7pCFMS--HyWHJq-O4KWye7muE9ye6LmWaKOF2-16g0"></script>
                </div>
            </td>
        </tr>
    </tbody>
  </table> -->




<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right">
    Website template from <a href="http://www.cs.berkeley.edu/~barron/">here</a>,  <a href="http://www.cs.cmu.edu/~dpathak/">here</a> and <a href="https://tairanhe.com/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('koopman_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('lego_manipulation_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('apo_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('scpo_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('guard_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('htcp_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('s3po_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('lego2dlfd_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('jpc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('maniploco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('parkour_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('mobile_aloha_abs');
</script>
</body>

</html>
